---
title: 分布式系统的原理性问题
author:
  name: superhsc
  link: https://github.com/maxpixelton
date: 2021-12-04 22:33:00 +0800
categories: [系统设计, 分布式]
tags:  [Architecture Design]
math: true
mermaid: true
---

## 案例背景

思考如下问题：

1. 如何设计一个支持海量商品存储的高扩展性架构 ？
2. 在做分库分表时，基于 Hash 取模和一致性 Hash 的数据分片是如何实现的 ？
3. 在电商大促时期，如何对热点商品数据做存储策略 ？
4. 强一致性和最终一致性的数据共识算法是如何实现的 ？

在分布式系统中，核心关注点包括：
- 存储；
- 分布；
- 复制；
- 相关协议和算法。

上述四个问题都与这些关注点相关。


## 案例分析

- 数据存储：在互联网业务场景下，为解决单台存储设备局限性，会把数据分布到多台存储节点，以实现数据水平扩展；
- 数据分片：把数据分布到多个节点，就会产生**数据分片**问题。
  数据分片就是**按照一定的规则将数据路由到相应的存储节点中，从而降低单存储节点带来的读写压力**。
  数据分片的常见实现方案有：
  - Hash （哈希分片）
  - Range（范围分片）
- 数据复制：明确如何分片之后，需要对数据进行复制；
- 数据副本：由数据复制产生，是分布式存储系统解决高可用的唯一手段，
  其中，众所周知的是**主从模式 master-slave**；在分布式存储系统中，通常会设置数据副本的主从节点，当主节点出现故障时，从节点可以替代主节点提供服务，从而保证业务正常运行。
  如何让从节点替代主节点又涉及**数据一致性问题**（只有在主从节点数据一致的情况下，才能进行主从替换）
- 数据一致性：关于数据一致性，通常要考虑**一致性强弱（即强一致性和最终一致性的问题）**；
- 协议算法：解决一致性问题，则要进行一系列的一致性协议，常见的有：

  - 两阶段提交协议（Two-Phrase Commit，2PC）；
  - Paxos 协议选举；
  - Raft 协议；
  - Gossip 协议。

所以分布式数据存储的问题可以分成：数据分片、数据复制，以及数据一致性带来的相关问题。

![分布式数据存储的问题](https://maxpixelton.github.io/images/assert/architecute/0401.png)

## 案例设计

假设你是某个电商网站的架构师，现在要将原有单点上百 G 商品做数据重构，存储到多个节点上，应该如何设计存储策略 ？

### 数据分片

因为是商品存储扩容的设计问题，很容易想到做数据的**分库分表**，也就是重新设计数据分片规则，常用的分片策略有两种：

- Hash（哈希）分片；
- Range（范围）分片。

#### Hash（哈希）分片

商品表包括主键、商品 ID、商品名称、所属品类和上架时间等字段。

如果以商品 ID 作为关键字进行分片，系统会**通过一个 Hash 函数计算商品 ID 的 Hash 值，然后取模**，就能得到对应的分片。模为 4 就表示系统一共有四个节点，每个节点作为一个分片。

假设 Hash 函数为 “商品 ID % 节点个数 4”，通过计算可以得到每个数据应该存入的节点：

- 计算结果为 0 时，数据存入节点 A；
- 计算结果为 1 时，数据存入节点 B；
- 计算结果为 2 时，数据存入节点 C；
- 计算结果为 3 时，数据存入节点 D。

![商品数据 Hash 存储](https://maxpixelton.github.io/images/assert/architecute/0402.png)

可以看出，Hash 分片有两个优点在于：

- 保证数据非常均匀地分布到多个分片上；
- 实现起来简单。

Hash 分片的缺点也很明显：

- 扩展性很差。因为分片的计算方式就是直接用节点取模，节点数量变动，就需要重新计算 Hash，进而导致大规模数据迁移工作。

#### 一致性 Hash 分片

**为解决 Hash 分片的缺点，既保证数据均匀分布，又保证扩展性，于是就有了<font color="red">一致性 Hash</font>** 。

一致性 Hash，指的是**将存储节点和数据都映射到一个首尾相连的哈希环上**。

- 存储节点一般根据 IP 地址进行 Hash 计算；
- 数据的存储位置是从数据映射在环上的位置开始，依照顺时针方向所找到的第一个存储节点。

在具体操作过程中，通常会**选择带有虚拟节点的一致性 Hash**。

假设在这个案例中将虚拟节点的数量设定为 10 个，就形成 10 个分片，而这 10 个分片构成了整个 Hash 空间。现在有方案目标：

- A 节点对应虚拟节点 0 ~ 3；
- B 节点对应虚拟节点 4 ~ 6；
- C 节点对应虚拟节点 7 ~ 8；
- D 节点对应虚拟节点 9。

同样根据哈希函数为 “商品 ID % 节点个数 10”得到每一个商品在 Hash 环上的位置，然后根据顺时针查找最近的存储节点，即数据实际映射的位置。计算结果为：

- 结果为 0 ~ 3 ，数据存入节点 A；
- 结果为 4 ~ 6 ，数据存入节点 B；
- 结果为 7 ~ 8 ，数据存入节点 C；
- 结果为 9 ，数据存储节点 D。

![商品一致性Hash存储](https://maxpixelton.github.io/images/assert/architecute/0403.png)

当新增一台服务器，即节点 E ，受影响的数据仅仅是新服务器到所处环空间中前一台服务器（即沿着逆时针方向的第一台服务器）之间的数据。

结合示例，只有商品 100 和商品 101 从节点 A 被移动到节点 E，其他节点的数据保持不变。此后，节点 A 只存储 Hash 值为 2 和 3 的商品，节点 E 存储 Hash 值为 0 和 1 的商品。

![商品数据迁移](https://maxpixelton.github.io/images/assert/architecute/0404.png)

由此可见，一致性 Hash 分片的优点：

1. **数据可以较为均匀地分配到各节点；**
2. **并发写入性能表现良好**；
3. **提升了稳定性**，节点加入和退出不会造成大规模数据迁移。

一致性 Hash 分配的缺点：

1. 无法避免单一热点问题。

   本质上 Hash 分片是一种静态的分片方式，必须要提前设定分片最大规模。

   某一数据被海量并发请求后，不论如何进行 Hash，数据也只能存在一个节点上，势必会带来热点请求问题。比如案例中如果某些商品卖得非常火爆，**通过 Hash 分片的方式很难针对热点商品做单独的架构设计。**

#### Range（范围）分片

为解决单一热点问题，可以做 **Range（范围）分片**。与 Hash 分片不同的是，Range 分片可以结合业务逻辑规则。

假设用 “Category（商品类目）” 作为关键字进行分片时，不是以统一的商品一级类目为标准，而是可以按照一、二、三级类目进行灵活分片。 比如，3C 品类，可以按照 3C 的三级品类设置分片；对于弱势品类，可以先按照一级品类进行分片，这样会让分片间数据更加平衡。

![0405](https://maxpixelton.github.io/images/assert/architecute/0405.png)

要达到这种灵活性，前提是要**有能力控制数据流向哪个分区**，一个简单的实现方式是：预先设定主键生成规则，根据规则进行数据分片路由，但这种方式会侵入商品各条线主数据的业务规则。

**更好的方式是基于分片元数据**（不过架构设计没有好坏，只有适合与否）。

基于分片元数据，就是**调用端在操作数据时，先问分片元数据系统数据在哪，然后根据得到的地址操作数据。**其中：

- 元数据中存储的是数据分片信息；
- 分片信息是数据分布情况。

在一个分布式存储系统中，承担数据调度功能的节点是**分片元数据**，当客户端收到请求后，会请求分片元数据服务，获取分片对应的实际节点地址，才能访问真正的数据。而请求分片元数据获取的信息也不仅仅只有数据分片信息，还包括数据量、读写 QPS 和分片副本的健康状态等。

这种方式的灵活性在于分片规则不固定，易扩展，但是高灵活性就会带来高复杂性，从存储的角度看，元数据也是数据，特殊之处在于它类似一个路由表，每一次请求都要访问它，所以分片元数据本身就要做到高可用。如果系统支持动态分片，那么分片信息的变更数据还要在节点之间进行同步，这又带来多副本之间的一致性问题，以此延伸出如何保证分片元数据服务的可用性和数据一致性？

### 数据一致性

最直接方式是专门给元数据做一个服务集群，通过一致性算法复制数据。

在实现方式上，就是将元数据服务的高可用和数据一致性问题转嫁给外围协调组件，如 [ETCD](https://etcd.io/) 集群，这样既保证了系统的可靠，数据同步的成本又比较低。

具体的架构实现：

1. 给分片元数据做集群服务，通过 ETCD 存储数据分片信息。

2. 每个数据存储实例节点定时向元数据服务集群同步心跳和分片信息。

3. 当调用端请求过来，元数据服务节点只需要做好高可用和缓存即可。

![0406](https://maxpixelton.github.io/images/assert/architecute/0406.png)

### 协议算法

在一致性共识算法和最终一致性共识算法方面提出类似的问题，比如， ETCD 是如何解决数据共识问题的？为什么要选择这种数据复制方式呢？

对于这类问题，你要从一致性算法原理层面解答，思路是：清楚 ETCD 的共识算法是什么，还有哪些常用的共识算法，以及为什么 ETCD 会做这样的选型。

ETCD 的共识算法是基于 Raft 协议实现的强一致性算法，同类的强一致性算法还有 Paxos。

为什么没有选择 Paxos 而选择了 Raft ？主要以下内容的理解：

- Paxos 算法解决了什么问题 ？
- Basic Paxos 算法工作流程是什么 ？
- Paxos 算法和 Raft 算法的区别又是什么？

#### Paxos

在分布式系统中，造成系统不可用的场景很多，比如服务器硬件损坏、网络数据丢包等问题，解决这些问题的根本思路是多副本，副本是分布式系统解决高可用的唯一手段，也就是主从模式，那么如何在保证一致性的前提下，系统的可用性，Paxos 就被用来解决这样的问题。

Paxos 又分为：

- Basic Paxos
- Multi Paxos。

因为它们实现复杂，工业界很少直接采用 Paxos 算法，所以 ETCD 选择了 Raft 算法 （在面试过程中，面试官容易在这里设置障碍，来对候选者做技术分层）。

Raft 是 Multi Paxos 的一种实现，是通过一切以领导者为准的方式，实现一系列值的共识，然而不是所有节点都能当选 Leader 领导者，Raft 算法对于 Leader 领导者的选举是有限制的，只有最全的日志节点才可以当选。正因为 ETCD 选择了 Raft，为工业界提供了可靠的工程参考，就有更多的工程实现选择基于 Raft，如 TiDB 就是基于 Raft 算法的优化。

如果把问题设计的极端一些，考察你对最终一致性算法的掌握，还可以有一种思路：分片元数据服务毕竟是一个中心化的设计思路，而且基于强一致性的共识机制还是可能存在性能的问题，有没有更好的架构思路呢？

既然要解决可用性的问题，根据 Base 理论，需要实现最终一致性，那么 Raft 算法就不适用了，因为 Raft 需要保证大多数节点正常运行后才能运行。这个时候，可以选择基于 Gossip 协议的实现方式。

Gossip 的协议原理有一种传播机制叫谣言传播，指的是当一个节点有了新数据后，这个节点就变成了活跃状态，并周期性地向其他节点发送新数据，直到所有的节点都存储了该条数据。这种方式达成的数据一致性是 “最终一致性”，即执行数据更新操作后，经过一定的时间，集群内各个节点所存储的数据最终会达成一致，很适合动态变化的分布式系统。

![0407](https://maxpixelton.github.io/images/assert/architecute/0407.png)

从图中你可以看到，节点 A 向节点 B、C 发送新数据，节点 B 收到新数据后，变成了活跃节点，然后节点 B 向节点 C、D 发送新数据。

到此，我们对一致性共识算法做个总结，共识算法的选择和数据副本数量的多少息息相关，如果副本少、参与共识的节点少，推荐采用广播方式，如 Paxos、Raft 等协议。如果副本多、参与共识的节点多，那就更适合采用 Gossip 这种最终一致性协议。

![0408](https://maxpixelton.github.io/images/assert/architecute/0408.png)

## 总结

总的来说，通过电商场景下商品的存储设计，一步步延伸出了分布式系统的数据存储、分片，与数据一致性等分布式问题，它们包含了分布式系统知识体系中最基础的理论，也是最复杂的问题。：

- 基于 Hash 取模、一致性 Hash 实现分库分表的解决方案，是你能否通过这第一关的关键；
- 因为分布式系统架构设计离不开系统可用性与一致性之间的权衡，所以你的解题思路要站在这两个技术点之上；
- 对于分布式系统中的一致性共识算法，如 Basic Paxos、Multi Paxos、Raft、Zab、Gossip
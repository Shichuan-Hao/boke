---
title: 关于 JVM 调优经历总结
author: Shaun
date: 2020-02-05 10:33:00 +0800
categories: [博客]
tags: [jvm]
pin: true
math: true
mermaid: true
---

## <font color=red>Q：JVM 究竟需不需要调优 ？</font>

首先，JVM 究竟需不需要调优，这是我们首要搞清楚的。

JVM 经过这么多年的发展和验证，整体是非常健壮的。个人认为 99% 的情况下，基本用不到 JVM 调优。

通常来说，我们的 JVM 参数配置大多要遵循 JVM 官方的建议，例如：
- `-XX:NewRatio=2`：年轻代：老年代 = 1：2
- `-XX:SurvivorRatio=8`：eden:survivor=8:1
- 堆内存设置为物理内存的 3/4 左右
- ......

JVM 参数推荐的默认值都是经过 JVM 团队反复测试和前人的充分验证得出的比较合理的值，因此一般情况下是比较靠谱和通用的，一般不会出现大问题。

更重要的是，大部分的应用 QPS 都不到 10，数据量不到几万，这种低压环境下，想让 JVM 出问题，说实话也挺难的。

更多情况遇到 JVM 出现异常应该是自己的代码 bug 导致的 OOM、CPU load 高、GC 频繁...而这些场景也基本上都是代码修复即可，通常不需要动 JVM。

不过，凡事无绝对，还有小部分场景，是可能需要用到 JVM 调优的（下文详细总结）。特别的，这里所说的 JVM 调优更多的是针对自己的业务场景对 JVM 参数进行优化调整，使其更适合自身的业务，而不是对 JVM 源码的改动。

<hr/>

## <font color=red>Q：JVM 调优没必要，使用性能更好垃圾回收器就能解决问题 ？</font>

在不考虑其他因素（面试...）的情况下，升级垃圾回收器确实是最有效的方式之一，比如：CMS 升级到 G1，甚至 ZGC。

这个非常容易理解，因为更高版本的垃圾回收器相当于是 JVM 开发人员对 JVM 做的优化，毕竟人家是专门做这个的，所以通常来说升级更高版本性能会有不少的提升。

目前，G1 已经开始逐渐应用开来，周围也有不少团队在 JDK8 中使用 G1，就我了解到的，还是存在不是问题的，不少小伙伴在不断进行参数的调整，而在 JDK11 中能优化成啥样还待验证。

ZGC 目前应用的还比较少，仅仅从对外公布的数据来看很好看，最大暂停时间不超过 10ms，甚至是 1ms，大家的期望都很高。但是从目前我收集到的一些资料来看，ZGC 也并不是银弹，已知的明显问题有：

1. 吞吐量与 G1 相比会有所下降，官方称最大不超过 15%。
2. ZGC 如果遇到非常高的对象分配速率（allocation rate）的话会跟不上，目前唯一有效的 "调优"方式是增大整个 GC 堆的大小来让 ZGC 有更大的喘息空间 - R 大与 ZGC 领队沟通后的原话。

而后，随着后续 ZGC 应用开来，后续一定会不断出现更多问题的。

整体而言，个人觉着 JVM 调优在某些场景下还是有必要的，比较有句话叫：没有最好的，只有最合适的。

<font color=red>从另外一个角度，如果直接回答升级垃圾回收器，别人可能也会赞同，但是这个话题可能就这样结束了，别人大概率也没有听到他想要的回答，所以，在这种情况下，可以回答升级垃圾收集器，但是不能只回答升级垃圾回收器。</font>

## <font color=red>Q：JVM 何时优化 ？</font>

切记过早优化。《计算机程序设计艺术》的作者高德纳（Donald Ervin Knuth）曾说过一句经典的话：

> The real problem is that programmers have spent far too much time worrying about efficiency in the wrong places and at the wrong times; premature optimization is the root of all evil (or at least most of it) in programming.<br/>
真正的问题是，程序猿在错误的地方和错误的时间花了太多的时间担心效率问题；过早的优化是编程中所有（或者至少是大部分）罪恶的根源。
{: .prompt-warning }

切记过早优化并不是说完全管，比较正确的做法应该是：<font color=red>给核心服务的一些重要 JVM 指标配上监控告警，当指标出现波动或者异常时，能及时介入排查。</font>


## <font color=red>Q：JVM 有哪些核心指标 ？合理范围应该是多少 ？</font>

这个问题没有统一的答案，因为每个服务对 AVG/TP999/TP9999 等性能指标的要求是不同的，因此合理的范围也不同。

经过收集资料和经验总结，对于普通的 Java 后端应用来说，我得出来一份相对合理的范围值（以下指标都是对于单台服务器来说）：
- `jvm.gc.time`：每分钟的GC耗时在1s以内，500ms以内尤佳
- `jvm.gc.meantime`：每次YGC耗时在100ms以内，50ms以内尤佳
- `jvm.fullgc.count`：FGC最多几小时1次，1天不到1次尤佳
- `jvm.fullgc.time`：每次FGC耗时在1s以内，500ms以内尤佳

一般而言，只要这几个指标正常，其他的一般不会有问题，如果其他地方出了问题，一般都会影响到这几个指标。

## <font color=red>Q：JVM 优化步骤 ？</font>

### 分析和定位当前系统的瓶颈

对于 JVM 的核心指标，关注的和常用工具如下：

#### CPU 指标

- 查看占用 CPU 最多的进程
- 查看占用 CPU 最多的线程
- 查看线程堆栈快照信息
- 分析代码执行热点
- 查看哪个代码占用 CPU 执行时间最长
- 查看每个方法占用 CPU 时间比例

常用的命令有：
```shell
// 显示系统各个进程的资源使用情况
top
// 查看某个进程中的线程占用情况
top -Hp pid
// 查看当前 Java 进程的线程堆栈信息
jstack pid
```

常用的工具有：__JProfiler__、__JVM Profiler__、__Arthas__ 等。

#### JVM 内存指标

- 查看当前 JVM 堆内存参数配置是否合理
- 查看堆中对象的统计信息
- 查看堆存储快照，分析内存的占用情况
- 查看堆各区域的内存增长是否正常
- 查看是哪个区域导致的GC
- 查看GC后能否正常回收到内存

常见的命令有：
```shell
// 查看当前的 JVM 参数配置
ps -ef | grep java
// 查看 Java 进程的配置信息，包括系统属性和JVM命令行标志
jinfo pid
// 输出 Java 进程当前的 gc 情况
jstat -gc pid
// 输出 Java 堆详细信息
jmap -heap pid
// 显示堆中对象的统计信息
jmap -histo:live pid
// 生成 Java 堆存储快照dump文件
jmap -F -dump:format=b,file=dumpFile.phrof pid
```

常见的工具：__Eclipse MAT__、__JConsole__ 等。

#### JVM GC 指标

- 查看每分钟 GC 时间是否正常
- 查看每分钟 YGC 次数是否正常
- 查看 FGC 次数是否正常
- 查看单次 FGC 时间是否正常
- 查看单次 GC 各阶段详细耗时，找到耗时严重的阶段
- 查看对象的动态晋升年龄是否正常

JVM 的 GC指标一般是从 GC 日志里面查看，默认的 GC 日志可能比较少，我们可以添加以下参数，来丰富我们的GC日志输出，方便我们定位问题。

GC日志常用 JVM 参数：
```shell
// 打印GC的详细信息
-XX:+PrintGCDetails
// 打印GC的时间戳
-XX:+PrintGCDateStamps
// 在GC前后打印堆信息
-XX:+PrintHeapAtGC
// 打印Survivor区中各个年龄段的对象的分布信息
-XX:+PrintTenuringDistribution
// JVM启动时输出所有参数值，方便查看参数是否被覆盖
-XX:+PrintFlagsFinal
// 打印GC时应用程序的停止时间
-XX:+PrintGCApplicationStoppedTime
// 打印在GC期间处理引用对象的时间（仅在PrintGCDetails时启用）
-XX:+PrintReferenceGC
```

上面就是我定位系统瓶颈的常用手段，大部分问题通过以上方式都能定位问题的原因，然后结合代码找到问题根源。

### 确定优化目标

定位出系统瓶颈后，在优化前先制定好优化的目标是什么，例如：

- 将 FGC 次数从每小时 1 次，降低到 1 天 1 次
- 将每分钟的GC耗时从 3s 降低到 500ms
- 将每次FGC耗时从5s降低到1s以内
- ...

### 制订优化方案

针对定位出的系统瓶颈制定相应的优化方案，常见的有：
- __代码bug__：升级修复bug。典型的有：死循环、使用无界队列。
- __不合理的JVM参数配置__：优化 JVM 参数配置。典型的有：年轻代内存配置过小、堆内存配置过小、元空间配置过小。

### 对比优化前后的指标，统计优化效果

### 持续观察和跟踪优化效果

### 如果还需要的话，重复以上步骤

## 经验总结

### metaspace 导致频繁 FGC 问题

服务环境：ParNew + CMS + JDK8

问题现象：服务频繁出现FGC

原因分析：

1. 首先查看 GC 日志，发现出现 FGC 的原因是 metaspace 空间不够，对应的 GC 日志：
    ```
    Full GC (Metadata GC Threshold)
    ```
2. 进一步查看日志发现元空间存在内存碎片化现象，对应 GC 日志：
    ```
    Metaspace  used 35337K, capacity 56242K, committed 56320K, reserved 1099776K
    ```
    这里面几个参数的意义：
    - used：已经使用的空间大小。
    - capacity：当前已经分配并且未释放的空间容量大小。
    - committed: 当前已经分配的空间大小。
    - reserved: 预留的空间大小。
    这面的 used 容易理解，reserved 在本例不重要可忽略，主要是 capacity 和 committed。
    
    如下图：

    ![Virtual Space]()

    元空间的分配以 chunk 为单位，当一个 ClassLoader 被垃圾回收时，所有属于它的空间（chunk）被释放，此时该 chunk 称为 Free Chunk，而 committed chunk 就是 capacity chunk 和 free chunk 之和。

    之所以说内存存在碎片化现象就是根据 used 和 capacity 的数据得来的，上面说了元空间的分配以 chunk 为单位，即使一个 ClassLoader 只加载1个类，也会独占整个 chunk，所以当出现 used 和 capacity 两者之差较大的时候，说明此时存在内存碎片化的情况。

    GC 日志如下：
    ```
    {Heap before GC invocations=0 (full 0):
    par new generation   total 314560K, used 141123K [0x00000000c0000000, 0x00000000d5550000, 0x00000000d5550000)
    eden space 279616K,  50% used [0x00000000c0000000, 0x00000000c89d0d00, 0x00000000d1110000)
    from space 34944K,   0% used [0x00000000d1110000, 0x00000000d1110000, 0x00000000d3330000)
    to   space 34944K,   0% used [0x00000000d3330000, 0x00000000d3330000, 0x00000000d5550000)
    concurrent mark-sweep generation total 699072K, used 0K [0x00000000d5550000, 0x0000000100000000, 0x0000000100000000)
    Metaspace       used 35337K, capacity 56242K, committed 56320K, reserved 1099776K
    class space    used 4734K, capacity 8172K, committed 8172K, reserved 1048576K
    1.448: [Full GC (Metadata GC Threshold) 1.448: [CMS: 0K->10221K(699072K), 0.0487207 secs] 141123K->10221K(1013632K), [Metaspace: 35337K->35337K(1099776K)], 0.0488547 secs] [Times: user=0.09 sys=0.00, real=0.05 secs] 
    Heap after GC invocations=1 (full 1):
    par new generation   total 314560K, used 0K [0x00000000c0000000, 0x00000000d5550000, 0x00000000d5550000)
    eden space 279616K,   0% used [0x00000000c0000000, 0x00000000c0000000, 0x00000000d1110000)
    from space 34944K,   0% used [0x00000000d1110000, 0x00000000d1110000, 0x00000000d3330000)
    to   space 34944K,   0% used [0x00000000d3330000, 0x00000000d3330000, 0x00000000d5550000)
    concurrent mark-sweep generation total 699072K, used 10221K [0x00000000d5550000, 0x0000000100000000, 0x0000000100000000)
    Metaspace       used 35337K, capacity 56242K, committed 56320K, reserved 1099776K
    class space    used 4734K, capacity 8172K, committed 8172K, reserved 1048576K
    }
    {Heap before GC invocations=1 (full 1):
    par new generation   total 314560K, used 0K [0x00000000c0000000, 0x00000000d5550000, 0x00000000d5550000)
    eden space 279616K,   0% used [0x00000000c0000000, 0x00000000c0000000, 0x00000000d1110000)
    from space 34944K,   0% used [0x00000000d1110000, 0x00000000d1110000, 0x00000000d3330000)
    to   space 34944K,   0% used [0x00000000d3330000, 0x00000000d3330000, 0x00000000d5550000)
    concurrent mark-sweep generation total 699072K, used 10221K [0x00000000d5550000, 0x0000000100000000, 0x0000000100000000)
    Metaspace       used 35337K, capacity 56242K, committed 56320K, reserved 1099776K
    class space    used 4734K, capacity 8172K, committed 8172K, reserved 1048576K
    1.497: [Full GC (Last ditch collection) 1.497: [CMS: 10221K->3565K(699072K), 0.0139783 secs] 10221K->3565K(1013632K), [Metaspace: 35337K->35337K(1099776K)], 0.0193983 secs] [Times: user=0.03 sys=0.00, real=0.02 secs] 
    Heap after GC invocations=2 (full 2):
    par new generation   total 314560K, used 0K [0x00000000c0000000, 0x00000000d5550000, 0x00000000d5550000)
    eden space 279616K,   0% used [0x00000000c0000000, 0x00000000c0000000, 0x00000000d1110000)
    from space 34944K,   0% used [0x00000000d1110000, 0x00000000d1110000, 0x00000000d3330000)
    to   space 34944K,   0% used [0x00000000d3330000, 0x00000000d3330000, 0x00000000d5550000)
    concurrent mark-sweep generation total 699072K, used 3565K [0x00000000d5550000, 0x0000000100000000, 0x0000000100000000)
    Metaspace       used 17065K, capacity 22618K, committed 35840K, reserved 1079296K
    class space    used 1624K, capacity 2552K, committed 8172K, reserved 1048576K
    }
    ```
    
    元空间主要适用于存放类的相关信息，而存在内存碎片化说明很可能创建了较多的类加载器，同时使用率较低。

    因此，当元空间出现内存碎片化时，我们会着重关注是不是创建了大量的类加载器。

3. 通过 dump 堆存储文件发现存在大量 DelegatingClassLoader。

    通过进一步分析，发现是由于反射导致创建大量 DelegatingClassLoader。其核心原理如下：

    在 JVM 上，最初是通过 JNI 调用来实现方法的反射调用，当 JVM 注意到通过反射经常访问某个方法时，它将生成字节码来执行相同的操作，称为膨胀（inflation）机制。如果使用字节码的方式，则会为该方法生成一个 DelegatingClassLoader，如果存在大量方法经常反射调用，则会导致创建大量 DelegatingClassLoader。

    > __反射调用频次达到多少才会从 JNI 转字节码 ？__<br/>默认是 15 次，可通过参数 `-Dsun.reflect.inflationThreshold` 进行控制，在小于该次数时会使用 JNI 的方式对方法进行调用，如果调用次数超过该次数就会使用字节码的方式生成方法调用。
    {: .prompt-tip }

4. 分析总结。反射调用导致创建大量 DelegatingClassLoader，占用了较大的元空间内存，同时存在内存碎片化现象，导致元空间利用率不高，从而较快达到阈值，触发 FGC。

5. 优化策略。

    1. 适当调大 metaspace 的空间大小。
    2. 优化不合理的反射调用。例如最常见的属性拷贝工具类 BeanUtils.copyProperties 可以使用 mapstruct 替换。

## 总结

当被面试官问到 JVM 调优时，完全可以按照本文的脉络回答：

- 首先表态如果使用合理的 JVM 参数配置，在大多数情况应该是不需要调优的，参考 #Q1
- 说明可能还是存在少量场景需要调优，我们可以对一些 JVM 核心指标配置监控告警，当出现波动时人为介入分析评估 —— 参考 #Q3
- 实际的调优例子，参考 #Q5
- 关于分析排查的，参考 #Q4